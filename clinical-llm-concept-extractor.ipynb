{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import wandb\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "from typing import List, Dict, Any\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from sqlalchemy import create_engine, text\n",
    "from dotenv import load_dotenv\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "def get_time_string(fmt=\"%m%d_%H%M\"):\n",
    "    return datetime.now().strftime(fmt)\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "config = {\n",
    "    \"wandb_project\": \"clinical-concept-extraction-and-prediction\",\n",
    "    \"wandb_entity\": \"llm-lab\",\n",
    "    \"wandb_run_name\": f\"{get_time_string}_concept-extraction-and-prediction\",\n",
    "    \"model_name\" : \"meta-llama/Meta-Llama-3.1-8B-Instruct\",\n",
    "    \"label_mapping\": {\n",
    "      \"ED\": 0,\n",
    "      \"Med-Surg\": 1, \n",
    "      \"Observation\": 2,\n",
    "      \"ICU\": 3,\n",
    "      \"Stepdown\": 4\n",
    "    },\n",
    "    \"location_mapping\": {\n",
    "      \"Corning Hospital 2e\": \"Med-Surg\",\n",
    "      \"Corning Hospital 2w\": \"Med-Surg\",\n",
    "      \"Corning Hospital Emergency Department\": \"ED\",\n",
    "      \"Corning Hospital Icu\": \"ICU\",\n",
    "      \"Cortland Hospital 2c\": \"Med-Surg\",\n",
    "      \"Cortland Hospital 2s\": \"Med-Surg\",\n",
    "      \"Cortland Hospital Icu\": \"ICU\",\n",
    "      \"Cortland Hospital Maternity\": \"Med-Surg\",\n",
    "      \"Guthrie Rph Towanda Med/Surg Unit\": \"Med-Surg\",\n",
    "      \"Guthrie Towanda Campus, Med/Surg Unit\": \"Med-Surg\",\n",
    "      \"Robert Packer Hospital 2Icu\": \"ICU\",\n",
    "      \"Robert Packer Hospital 4w\": \"Observation\",\n",
    "      \"Robert Packer Hospital 6m\": \"Med-Surg\",\n",
    "      \"Robert Packer Hospital 6w\": \"Med-Surg\",\n",
    "      \"Robert Packer Hospital 7Icu\": \"Stepdown\",\n",
    "      \"Robert Packer Hospital 7m\": \"Med-Surg\",\n",
    "      \"Robert Packer Hospital 7nw\": \"Med-Surg\",\n",
    "      \"Robert Packer Hospital 8nw\": \"Med-Surg\",\n",
    "      \"Robert Packer Hospital 9sw\": \"Med-Surg\",\n",
    "      \"Robert Packer Hospital Emergency Department\": \"ED\",\n",
    "      \"Robert Packer Hospital JC\": \"Med-Surg\",\n",
    "      \"RPH 6 NORTHWEST\": \"Med-Surg\",\n",
    "      \"RPH 6 Southwest\": \"Med-Surg\",\n",
    "      \"Troy Community Hospital 1m\": \"Med-Surg\",\n",
    "      \"Robert Packer Hospital Preprocedure\": \"Med-Surg\",\n",
    "      \"Robert Packer Hospital Recovery\": \"Med-Surg\",\n",
    "      \"Guthrie Rph Towanda Emergency Department\": \"ED\",\n",
    "      \"Guthrie Rph Towanda Skilled Nursing Unit\": \"Med-Surg\",\n",
    "      \"Guthrie Towanda Campus, Emergency Department\": \"ED\",\n",
    "      \"Guthrie Towanda Campus, Skilled Nursing Unit\": \"Med-Surg\",\n",
    "      \"Towanda Memorial Hospital Skilled Nursing\": \"Med-Surg\",\n",
    "      \"Troy Community Hospital Emergency Dept\": \"ED\",\n",
    "      \"Troy Community Hospital Preprocedure\": \"Med-Surg\",\n",
    "      \"Cortland Hospital Emergency Department\": \"ED\",\n",
    "      \"Cortland Hospital Preprocedure\": \"Med-Surg\",\n",
    "      \"Cortland Hospital Recovery\": \"Med-Surg\",\n",
    "      \"Guthrie Rph Towanda Acute Rehab Unit\": \"Med-Surg\",\n",
    "      \"Cortland Hospital Ultrasound\": \"Med-Surg\",\n",
    "      \"CH Recovery\": \"Med-Surg\",\n",
    "      \"Corning Hospital Observation Unit\": \"Observation\",\n",
    "      \"Robert Packer Hospital\": \"Med-Surg\",\n",
    "      \"Guthrie Towanda Campus, Preprocedure\": \"Med-Surg\"\n",
    "    },\n",
    "    \"batch_size\": 10, # Number of encounters to fetch and process in each batch\n",
    "    \"max_length\": 2048,\n",
    "}\n",
    "\n",
    "# Initialize wandb\n",
    "wandb.login(key=os.getenv('WANDB_API_KEY'))\n",
    "wandb.init(project=config[\"wandb_project\"], entity=config[\"wandb_entity\"], name=config[\"wandb_run_name\"])\n",
    "\n",
    "# Connect to the source PostgreSQL database\n",
    "source_db_url = f\"postgresql+psycopg2://{os.getenv('ARTISIGHT_DB_USER')}:{os.getenv('ARTISIGHT_DB_PASSWORD')}@{os.getenv('ARTISIGHT_DB_HOST')}:{os.getenv('ARTISIGHT_DB_PORT')}/{os.getenv('ARTISIGHT_DB_NAME')}\"\n",
    "source_engine = create_engine(source_db_url)\n",
    "\n",
    "# Load the local LLM model\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(config[\"model_name\"])\n",
    "model = AutoModelForCausalLM.from_pretrained(config[\"model_name\"])\n",
    "\n",
    "def fetch_batch_encounter_data() -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Fetch encounter data for a batch of encounters from the source database.\n",
    "    \"\"\"\n",
    "    query = text(f\"\"\"\n",
    "    WITH ranked_encounters AS (\n",
    "        SELECT \n",
    "            encounters.id AS encounter_id,\n",
    "            ROW_NUMBER() OVER (ORDER BY encounters.id) AS row_num\n",
    "        FROM data_service_fhir_app_encounter encounters\n",
    "        WHERE encounters.document ->> 'status' = 'finished'\n",
    "    )\n",
    "    SELECT \n",
    "        encounters.id AS encounter_id,\n",
    "        encounters.fhir_id AS fhir_encounter_id,\n",
    "        encounters.patient_id,\n",
    "        patients.fhir_id AS fhir_patient_id,\n",
    "        ((encounters.document#>> '{{}}')::jsonb->'period'->>'start')::timestamp AS encounter_start,\n",
    "        ((encounters.document#>> '{{}}')::jsonb->'period'->>'end')::timestamp AS encounter_end,\n",
    "        encounters.document ->> 'status' AS encounter_status,\n",
    "        docs.id AS documentreference_id,\n",
    "        docs.fhir_id AS fhir_documentreference_id,\n",
    "        binaries.id AS binary_id,\n",
    "        docs.document -> 'type' ->> 'text' AS note_type,\n",
    "        (docs.document ->> 'date')::timestamp AS note_date,\n",
    "        binaries.last_updated AS note_last_updated,\n",
    "        encounters.document -> 'location' as locations,\n",
    "        (encounters.document -> 'location' -> -1 -> 'location' ->> 'display') AS current_location,\n",
    "        CASE \n",
    "            WHEN jsonb_array_length(encounters.document -> 'location') > 1 \n",
    "            THEN (encounters.document -> 'location' -> -2 -> 'location' ->> 'display')\n",
    "            ELSE NULL\n",
    "        END AS previous_location,\n",
    "        binaries.processed_data\n",
    "    FROM ranked_encounters\n",
    "    JOIN data_service_fhir_app_encounter encounters ON ranked_encounters.encounter_id = encounters.id\n",
    "    JOIN data_service_fhir_app_documentreference_encounter docref ON encounters.id = docref.encounter_id\n",
    "    JOIN data_service_fhir_app_documentreference docs ON docref.documentreference_id = docs.id\n",
    "    JOIN data_service_fhir_app_documentreference_binary docbin ON docs.id = docbin.documentreference_id\n",
    "    JOIN data_service_fhir_app_binary binaries ON docbin.binary_id = binaries.id\n",
    "    JOIN data_service_fhir_app_patient patients ON encounters.patient_id = patients.id\n",
    "    WHERE binaries.processed_data IS NOT NULL\n",
    "        AND docs.document ->> 'docStatus' IN ('final', 'amended')\n",
    "        AND docs.document -> 'type' ->> 'text' IN ('Progress Notes', 'Consults', 'H&P')\n",
    "        AND ranked_encounters.row_num <= {config['batch_size']}\n",
    "    ORDER BY encounters.id, note_date\n",
    "    \"\"\")\n",
    "    \n",
    "    with source_engine.connect() as connection:\n",
    "        result = connection.execute(query)\n",
    "        df = pd.DataFrame(result.fetchall(), columns=result.keys())\n",
    "    return df\n",
    "\n",
    "def preprocess_encounters(df_encounters: pd.DataFrame) -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Preprocess encounter data to generate next location prediction samples.\n",
    "    \"\"\"\n",
    "    all_outputs = []\n",
    "    for encounter_id, df_encounter in df_encounters.groupby(\"encounter_id\"):\n",
    "        try:\n",
    "            df_encounter = df_encounter.sort_values(\"note_date\")\n",
    "            \n",
    "            locations = json.loads(df_encounter[\"locations\"].iloc[0])\n",
    "            if not locations or len(locations) < 2:\n",
    "                continue\n",
    "\n",
    "            # Create a list of location changes with timestamps\n",
    "            location_changes = []\n",
    "            for loc in locations:\n",
    "                if \"period\" in loc and \"start\" in loc[\"period\"]:\n",
    "                    location_changes.append({\n",
    "                        \"location\": loc[\"location\"][\"display\"],\n",
    "                        \"timestamp\": pd.to_datetime(loc[\"period\"][\"start\"]).tz_localize(None)\n",
    "                    })\n",
    "            \n",
    "            # Sort location changes by timestamp\n",
    "            location_changes.sort(key=lambda x: x[\"timestamp\"])\n",
    "\n",
    "            # Find transitions between different locations\n",
    "            transitions = []\n",
    "            for i in range(len(location_changes) - 1):\n",
    "                if location_changes[i][\"location\"] != location_changes[i+1][\"location\"]:\n",
    "                    transitions.append((i, i+1))\n",
    "\n",
    "            if not transitions:\n",
    "                continue  # No transitions between different locations found\n",
    "\n",
    "            outputs = []\n",
    "            for start_index, end_index in transitions:\n",
    "                current_location = location_changes[start_index][\"location\"]\n",
    "                next_location = location_changes[end_index][\"location\"]\n",
    "                \n",
    "                if next_location not in config[\"location_mapping\"] and next_location not in config[\"location_exclusions\"]:\n",
    "                    # print(f\"Warning: Location {next_location} not in location mapping, please update the mapping to include transitions to this location\")\n",
    "                    continue\n",
    "                elif next_location in config[\"location_exclusions\"]:\n",
    "                    continue\n",
    "                \n",
    "                label = config[\"location_mapping\"][next_location]\n",
    "                \n",
    "                transition_time = location_changes[end_index][\"timestamp\"]\n",
    "\n",
    "                # Filter notes up to the transition time\n",
    "                notes = df_encounter[\n",
    "                    (df_encounter[\"note_type\"].isin([\"H&P\", \"Progress Notes\", \"Consults\"])) &\n",
    "                    (df_encounter[\"note_date\"] <= transition_time)\n",
    "                ]\n",
    "\n",
    "                if notes.empty:\n",
    "                    continue\n",
    "\n",
    "                input_text = assemble_notes(notes)\n",
    "                \n",
    "                outputs.append({\n",
    "                    \"patient_id\": df_encounter[\"patient_id\"].iloc[0],\n",
    "                    \"encounter_id\": encounter_id,\n",
    "                    \"encounter_start\": df_encounter[\"encounter_start\"].iloc[0],\n",
    "                    \"encounter_end\": df_encounter[\"encounter_end\"].iloc[0],\n",
    "                    \"input\": input_text,\n",
    "                    \"label\": label,\n",
    "                    \"label_id\": config[\"label_mapping\"][label],\n",
    "                    \"db_record_last_updated\": notes[\"note_last_updated\"].iloc[-1],\n",
    "                    \"current_location\": current_location,\n",
    "                    \"current_time\": transition_time,\n",
    "                    \"next_location\": next_location,\n",
    "                    \"transition_time\": transition_time\n",
    "                })\n",
    "\n",
    "            all_outputs.extend(outputs)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing encounter {encounter_id}: {e}\")\n",
    "    \n",
    "    return all_outputs\n",
    "\n",
    "def assemble_notes(df_encounter: pd.DataFrame) -> str:\n",
    "    \"\"\"Assemble notes for an encounter.\"\"\"\n",
    "    inp = \"\"\n",
    "    hp_note = df_encounter[df_encounter[\"note_type\"] == \"H&P\"][\"processed_data\"]\n",
    "    progress_consults = df_encounter[df_encounter[\"note_type\"].isin([\"Progress Notes\", \"Consults\"])][\"processed_data\"]\n",
    "\n",
    "    if len(progress_consults) > 1:\n",
    "        for note_id in (-1, -2):\n",
    "            if abs(note_id) <= len(progress_consults):\n",
    "                note = progress_consults.iloc[note_id]\n",
    "                inp += note + \"\\n\"\n",
    "    elif len(progress_consults) > 0:\n",
    "        inp += progress_consults.iloc[-1] + \"\\n\"\n",
    "\n",
    "    if len(hp_note) > 0:\n",
    "        inp += hp_note.iloc[0]\n",
    "\n",
    "    return inp\n",
    "\n",
    "def extract_clinical_concepts(patient_data: str) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Extract structured clinical concepts from unstructured patient data.\n",
    "    \"\"\"\n",
    "    # TODO: - [ ] Enhance this prompt.\n",
    "    prompt = f\"\"\"\n",
    "    Extract clinical concepts from the following patient data:\n",
    "    {patient_data}\n",
    "    \n",
    "    Output the extracted concepts in JSON format.\n",
    "    \"\"\"\n",
    "    \n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "    outputs = model.generate(**inputs, max_length=500)\n",
    "    extracted_concepts = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "    try:\n",
    "        return json.loads(extracted_concepts)\n",
    "    except json.JSONDecodeError:\n",
    "        print(\"Error: Unable to parse extracted concepts as JSON.\")\n",
    "        return {}\n",
    "\n",
    "def predict_next_location(extracted_concepts: Dict[str, Any]) -> str:\n",
    "    \"\"\"\n",
    "    Predict the next location for the patient encounter based on extracted concepts.\n",
    "    \"\"\"\n",
    "    # TODO: - [ ] Enhance this prompt.\n",
    "    prompt = f\"\"\"\n",
    "    Given the following extracted clinical concepts:\n",
    "    {json.dumps(extracted_concepts, indent=2)}\n",
    "    \n",
    "    Predict the most likely next location for this patient encounter.\n",
    "    Choose from: {', '.join(config['location_mapping'].values())}\n",
    "    \"\"\"\n",
    "    \n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "    outputs = model.generate(**inputs, max_length=config[\"max_length\"])\n",
    "    predicted_location = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "    return predicted_location.strip()\n",
    "\n",
    "def process_batch(preprocessed_data: List[Dict[str, Any]]) -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Process a batch of preprocessed encounter data.\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    for sample in preprocessed_data:\n",
    "        extracted_concepts = extract_clinical_concepts(sample['input'])\n",
    "        predicted_location = predict_next_location(extracted_concepts)\n",
    "        \n",
    "        result = {\n",
    "            \"encounter_id\": sample['encounter_id'],\n",
    "            \"current_location\": sample['current_location'],\n",
    "            \"actual_next_location\": sample['next_location'],\n",
    "            \"predicted_next_location\": predicted_location,\n",
    "            \"num_extracted_concepts\": len(extracted_concepts),\n",
    "            \"is_correct\": predicted_location == sample['label']\n",
    "        }\n",
    "        results.append(result)\n",
    "        \n",
    "        # Log to wandb\n",
    "        wandb.log({\n",
    "            \"encounter_id\": sample['encounter_id'],\n",
    "            \"num_extracted_concepts\": len(extracted_concepts),\n",
    "            \"actual_next_location\": sample['label'],\n",
    "            \"predicted_next_location\": predicted_location,\n",
    "            \"is_correct\": result['is_correct']\n",
    "        })\n",
    "    \n",
    "    return results\n",
    "\n",
    "def analyze_results(results: List[Dict[str, Any]]):\n",
    "    \"\"\"\n",
    "    Analyze and visualize the results.\n",
    "    \"\"\"\n",
    "    df_results = pd.DataFrame(results)\n",
    "    \n",
    "    # Calculate accuracy\n",
    "    accuracy = df_results['is_correct'].mean()\n",
    "    \n",
    "    # Calculate confusion matrix\n",
    "    cm = confusion_matrix(df_results['actual_next_location'], df_results['predicted_next_location'], labels=list(config['location_mapping'].values()))\n",
    "    \n",
    "    # Plot confusion matrix\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(config['location_mapping'].values()))\n",
    "    plt.xticks(tick_marks, config['location_mapping'].values(), rotation=45)\n",
    "    plt.yticks(tick_marks, config['location_mapping'].values())\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('Actual Location')\n",
    "    plt.xlabel('Predicted Location')\n",
    "    \n",
    "    # Log plot to wandb\n",
    "    wandb.log({\"confusion_matrix\": wandb.Image(plt)})\n",
    "    \n",
    "    # Log statistics to wandb\n",
    "    wandb.log({\n",
    "        \"accuracy\": accuracy,\n",
    "        \"num_samples\": len(df_results)\n",
    "    })\n",
    "    \n",
    "# Main execution\n",
    "all_results = []\n",
    "batch_number = 1\n",
    "\n",
    "while True:\n",
    "    print(f\"Processing batch {batch_number}\")\n",
    "    \n",
    "    # Fetch a batch of encounter data\n",
    "    df_batch = fetch_batch_encounter_data()\n",
    "    \n",
    "    if df_batch.empty:\n",
    "        print(\"No more encounters to process.\")\n",
    "        break\n",
    "    \n",
    "    # Preprocess the batch\n",
    "    preprocessed_data = preprocess_encounters(df_batch)\n",
    "    \n",
    "    # Process the batch\n",
    "    batch_results = process_batch(preprocessed_data)\n",
    "    all_results.extend(batch_results)\n",
    "    \n",
    "    print(f\"Processed {len(batch_results)} samples in batch {batch_number}\")\n",
    "    batch_number += 1\n",
    "\n",
    "# Analyze results\n",
    "analyze_results(all_results)\n",
    "\n",
    "# Close wandb run\n",
    "wandb.finish()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
